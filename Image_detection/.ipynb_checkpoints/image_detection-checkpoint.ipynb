{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6fd0731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3740dd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roufa/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "from torchvision.transforms import functional as FT\n",
    "from torchvision import transforms as T\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, sampler, random_split, Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2e6a508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "from torchvision.utils import draw_bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ea950cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import albumentations as A  \n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66a01d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(train=False):\n",
    "    if train:\n",
    "        transform = A.Compose([\n",
    "            A.Resize(100, 100), # our input size can be 600px\n",
    "            A.HorizontalFlip(p=0.3),\n",
    "            A.VerticalFlip(p=0.3),\n",
    "            A.RandomBrightnessContrast(p=0.1),\n",
    "            A.ColorJitter(p=0.1),\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='coco'))\n",
    "    else:\n",
    "        transform = A.Compose([\n",
    "            A.Resize(100, 100), # our input size can be 600px\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='coco'))\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ef15a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Antsandbees(datasets.VisionDataset):\n",
    "    def __init__(self, root, json_path, transforms=None):\n",
    "        super().__init__(Antsandbees)\n",
    "        \n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.coco = COCO(json_path) # annotatiosn stored here\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        self.ids = [id for id in self.ids if (len(self._load_target(id)) > 0)]\n",
    "        #print(self.ids,\"iddddddddddddddd\")\n",
    "         \n",
    "    def _load_image(self, id: int):\n",
    "        path = self.coco.loadImgs(id)[0]['file_name']\n",
    "        image = cv2.imread(os.path.join(self.root, \"Sample_Data/images\", path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return image\n",
    "    \n",
    "    def _load_target(self, id):\n",
    "        return self.coco.loadAnns(self.coco.getAnnIds(id))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        id = self.ids[index]\n",
    "        image = self._load_image(id)\n",
    "        target = self._load_target(id)\n",
    "        target = copy.deepcopy(self._load_target(id))\n",
    "        \n",
    "        boxes = [t['bbox'] + [t['category_id']] for t in target]\n",
    "#         print(boxes)\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(image=image, bboxes=boxes)\n",
    "#             print(transformed)\n",
    "        image = transformed['image']\n",
    "        boxes = transformed['bboxes']\n",
    "#         print(boxes)\n",
    "        \n",
    "        new_boxes = []\n",
    "        for box in boxes:\n",
    "            xmin = box[0]\n",
    "            xmax = xmin + box[2]\n",
    "            ymin = box[1]\n",
    "            ymax = ymin + box[3]\n",
    "            new_boxes.append([xmin, ymin, xmax, ymax])\n",
    "        \n",
    "#         print(\"New boxes :\", new_boxes)\n",
    "        boxes = torch.tensor(new_boxes, dtype=torch.float32)\n",
    "        targ = {} \n",
    "        targ['boxes'] = boxes\n",
    "        targ['labels'] = torch.tensor([t['category_id'] for t in target], dtype=torch.int64)\n",
    "        targ['image_id'] = torch.tensor([t['image_id'] for t in target])\n",
    "        targ['area'] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        targ['iscrowd'] = torch.tensor([t['iscrowd'] for t in target], dtype=torch.int64)\n",
    "        return image.div(255), targ\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ids)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "299f28e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home/roufa/Desktop/interview/pytorch/Image_detection'\n",
    "Json_file = \"Sample_Data/2.coco_annotations/output.json\"\n",
    "transforms = get_transforms()\n",
    "\n",
    "Json_path = os.path.join(dataset_path, Json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97c50c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/roufa/Desktop/interview/pytorch/Image_detection/Sample_Data/2.coco_annotations/output.json'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Json_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2194ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Antsandbees(dataset_path, Json_path, transforms= transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c80c776d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset Antsandbees\n",
       "    Number of datapoints: 10\n",
       "    Root location: /home/roufa/Desktop/interview/pytorch/Image_detection\n",
       "    Compose([\n",
       "  Resize(always_apply=False, p=1, height=100, width=100, interpolation=1),\n",
       "  ToTensorV2(always_apply=True, p=1.0, transpose_mask=False),\n",
       "], p=1.0, bbox_params={'format': 'coco', 'label_fields': None, 'min_area': 0.0, 'min_visibility': 0.0, 'check_each_transform': True}, keypoint_params=None, additional_targets={})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aff4f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "223ce075",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.detection.fasterrcnn_resnet50_fpn()\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "num_classes = 2\n",
    "\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34e631f",
   "metadata": {},
   "source": [
    "Modifying the model to add a different backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e191beda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backbone =  torchvision.models.mobilenet_v2().features\n",
    "# backbone.out_channels = 1280\n",
    "\n",
    "\n",
    "# anchor_generator = AnchorGenerator(sizes=((32,64,128,256,512),),aspect_ratios=((0.5,1.0,2.0),))\n",
    "# roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "#                                                 output_size=7,\n",
    "#                                                 sampling_ratio=2)\n",
    "\n",
    "\n",
    "# model = FasterRCNN(backbone,\n",
    "#                    num_classes=2,\n",
    "#                    rpn_anchor_generator=anchor_generator,\n",
    "#                    box_roi_pool=roi_pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3db85fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65bb895e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset Antsandbees\n",
       "    Number of datapoints: 10\n",
       "    Root location: /home/roufa/Desktop/interview/pytorch/Image_detection\n",
       "    Compose([\n",
       "  Resize(always_apply=False, p=1, height=100, width=100, interpolation=1),\n",
       "  ToTensorV2(always_apply=True, p=1.0, transpose_mask=False),\n",
       "], p=1.0, bbox_params={'format': 'coco', 'label_fields': None, 'min_area': 0.0, 'min_visibility': 0.0, 'check_each_transform': True}, keypoint_params=None, additional_targets={})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1139f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0a0cba13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': tensor([[[ 98, 105, 112,  ...,  92,  92,  92],\n",
      "         [ 98, 105, 113,  ...,  90,  91,  87],\n",
      "         [103, 106, 112,  ...,  95,  91,  90],\n",
      "         ...,\n",
      "         [163, 167, 163,  ..., 125, 126, 126],\n",
      "         [161, 161, 162,  ..., 125, 124, 122],\n",
      "         [153, 156, 155,  ..., 122, 121, 117]],\n",
      "\n",
      "        [[121, 124, 129,  ..., 110, 105,  99],\n",
      "         [124, 127, 131,  ..., 110, 107, 107],\n",
      "         [126, 132, 134,  ..., 112, 108, 107],\n",
      "         ...,\n",
      "         [159, 158, 159,  ..., 126, 125, 126],\n",
      "         [157, 156, 155,  ..., 124, 123, 121],\n",
      "         [152, 155, 156,  ..., 121, 120, 116]],\n",
      "\n",
      "        [[107, 108, 114,  ...,  98,  95,  91],\n",
      "         [113, 114, 119,  ...,  98,  96,  98],\n",
      "         [118, 119, 121,  ..., 104, 100,  99],\n",
      "         ...,\n",
      "         [148, 149, 150,  ..., 120, 120, 118],\n",
      "         [145, 150, 149,  ..., 119, 118, 116],\n",
      "         [143, 144, 148,  ..., 116, 116, 111]]], dtype=torch.uint8), 'bboxes': [(13.600000000000001, 20.266666666666666, 12.399999999999999, 31.46666666666667, 0), (33.800000000000004, 20.8, 8.199999999999996, 21.599999999999998, 0), (29.4, 38.93333333333333, 8.600000000000001, 20.53333333333334, 0), (42.8, 15.2, 29.200000000000003, 64.8, 0), (77.8, 40.266666666666666, 13.600000000000009, 13.866666666666667, 0)]}{'image': tensor([[[ 81,  81,  82,  ...,  79,  78,  78],\n",
      "         [ 82,  81,  83,  ...,  79,  79,  76],\n",
      "         [ 80,  81,  83,  ...,  77,  76,  77],\n",
      "         ...,\n",
      "         [ 87,  86,  88,  ...,  30, 176,   0],\n",
      "         [ 86,  86,  87,  ..., 174,   4,   1],\n",
      "         [ 86,  86,  87,  ...,   3,   0,   1]],\n",
      "\n",
      "        [[152, 152, 153,  ..., 147, 146, 149],\n",
      "         [152, 152, 154,  ..., 151, 148, 146],\n",
      "         [152, 152, 154,  ..., 147, 147, 148],\n",
      "         ...,\n",
      "         [160, 159, 161,  ...,  30, 176,   1],\n",
      "         [159, 159, 159,  ..., 170,   0,   5],\n",
      "         [159, 159, 160,  ...,  10,   5,  17]],\n",
      "\n",
      "        [[234, 234, 235,  ..., 230, 229, 231],\n",
      "         [234, 234, 236,  ..., 233, 230, 228],\n",
      "         [234, 234, 236,  ..., 232, 229, 230],\n",
      "         ...,\n",
      "         [239, 238, 238,  ...,  30, 176,   5],\n",
      "         [236, 238, 238,  ..., 168,   0,   5],\n",
      "         [236, 238, 239,  ...,  17,  12,  37]]], dtype=torch.uint8), 'bboxes': [(13.932291666666666, 23.6328125, 70.57291666666667, 45.3125, 0)]}\n",
      "\n",
      "{'image': tensor([[[ 98,  96,  90,  ..., 188, 185, 183],\n",
      "         [112, 110, 108,  ..., 189, 185, 185],\n",
      "         [124, 125, 125,  ..., 189, 185, 185],\n",
      "         ...,\n",
      "         [ 39,  37,  34,  ...,  71,  72,  78],\n",
      "         [ 35,  36,  36,  ...,  70,  73,  78],\n",
      "         [ 30,  31,  36,  ...,  69,  71,  77]],\n",
      "\n",
      "        [[ 83,  81,  75,  ..., 171, 169, 168],\n",
      "         [ 98,  96,  91,  ..., 172, 169, 169],\n",
      "         [109, 110, 106,  ..., 171, 169, 169],\n",
      "         ...,\n",
      "         [ 30,  27,  27,  ..., 130, 131, 134],\n",
      "         [ 26,  27,  29,  ..., 129, 131, 132],\n",
      "         [ 25,  26,  29,  ..., 129, 129, 131]],\n",
      "\n",
      "        [[ 78,  76,  70,  ..., 164, 170, 168],\n",
      "         [ 89,  87,  83,  ..., 165, 170, 169],\n",
      "         [102, 103, 100,  ..., 169, 169, 169],\n",
      "         ...,\n",
      "         [ 22,  20,  21,  ..., 144, 142, 145],\n",
      "         [ 19,  20,  23,  ..., 143, 143, 144],\n",
      "         [ 21,  22,  23,  ..., 140, 141, 143]]], dtype=torch.uint8), 'bboxes': [(33.800000000000004, 33.60433604336043, 29.599999999999994, 20.596205962059614, 0), (3.0, 57.45257452574526, 92.39999999999999, 38.75338753387534, 0)]}\n",
      "{'image': tensor([[[ 47,  49,  48,  ...,  51,  51,  65],\n",
      "         [ 49,  51,  51,  ...,  42,  45,  59],\n",
      "         [ 55,  55,  57,  ...,  30,  39,  52],\n",
      "         ...,\n",
      "         [102,  91,  78,  ...,  79, 127, 154],\n",
      "         [108,  97,  83,  ..., 149, 149, 144],\n",
      "         [115, 104,  88,  ..., 130, 130, 124]],\n",
      "\n",
      "        [[141, 144, 142,  ..., 130, 131, 135],\n",
      "         [143, 145, 145,  ..., 123, 124, 130],\n",
      "         [146, 146, 148,  ..., 114, 117, 127],\n",
      "         ...,\n",
      "         [ 92,  84,  75,  ...,  68, 119, 152],\n",
      "         [ 96,  88,  78,  ..., 147, 147, 147],\n",
      "         [101,  95,  81,  ..., 136, 134, 133]],\n",
      "\n",
      "        [[143, 146, 148,  ..., 130, 127, 135],\n",
      "         [143, 145, 149,  ..., 130, 129, 132],\n",
      "         [147, 147, 149,  ..., 125, 127, 130],\n",
      "         ...,\n",
      "         [ 67,  66,  60,  ...,  62,  94, 114],\n",
      "         [ 72,  71,  64,  ..., 125, 128, 124],\n",
      "         [ 78,  78,  67,  ..., 118, 123, 123]]], dtype=torch.uint8), 'bboxes': [(46.800000000000004, 29.83425414364641, 24.79999999999999, 27.071823204419886, 0)]}\n",
      "{'image': tensor([[[205, 213, 205,  ..., 101,  96,  79],\n",
      "         [188, 209, 194,  ..., 120, 105,  95],\n",
      "         [192, 188, 183,  ..., 133, 121, 114],\n",
      "         ...,\n",
      "         [191, 149, 135,  ..., 229, 192, 248],\n",
      "         [191, 159, 148,  ..., 232, 219, 222],\n",
      "         [204, 189, 189,  ..., 216, 235, 216]],\n",
      "\n",
      "        [[196, 199, 189,  ...,  92,  90,  82],\n",
      "         [179, 191, 182,  ..., 107,  93,  92],\n",
      "         [173, 176, 177,  ..., 120, 111, 106],\n",
      "         ...,\n",
      "         [166, 130, 119,  ..., 219, 184, 239],\n",
      "         [166, 143, 128,  ..., 219, 206, 211],\n",
      "         [181, 175, 167,  ..., 201, 220, 202]],\n",
      "\n",
      "        [[155, 164, 147,  ...,  56,  54,  53],\n",
      "         [131, 150, 142,  ...,  70,  59,  57],\n",
      "         [123, 134, 139,  ...,  77,  73,  70],\n",
      "         ...,\n",
      "         [125,  93,  86,  ..., 192, 150, 203],\n",
      "         [124, 102,  91,  ..., 181, 167, 176],\n",
      "         [140, 126, 126,  ..., 159, 181, 165]]], dtype=torch.uint8), 'bboxes': [(47.8, 30.930930930930934, 35.8, 57.35735735735735, 0)]}\n",
      "{'image': tensor([[[ 22,  14,  17,  ...,   8,  11,  10],\n",
      "         [ 15,  26,  16,  ...,   7,   5,  14],\n",
      "         [ 20,  10,  18,  ...,   8,  10,  10],\n",
      "         ...,\n",
      "         [170, 184, 173,  ..., 184, 178, 187],\n",
      "         [173, 165, 183,  ..., 187, 175, 193],\n",
      "         [181, 184, 183,  ..., 191, 173, 192]],\n",
      "\n",
      "        [[ 23,  19,  19,  ...,  22,  10,  13],\n",
      "         [ 25,  23,  18,  ...,  14,  15,  11],\n",
      "         [ 18,  21,  18,  ...,  11,  16,  12],\n",
      "         ...,\n",
      "         [206, 207, 199,  ..., 233, 209, 217],\n",
      "         [206, 213, 217,  ..., 231, 207, 211],\n",
      "         [215, 223, 213,  ..., 224, 215, 226]],\n",
      "\n",
      "        [[ 19,  19,  18,  ...,  14,   8,   8],\n",
      "         [ 22,  15,  13,  ...,  12,  11,   8],\n",
      "         [ 16,  17,  14,  ...,  11,   9,  13],\n",
      "         ...,\n",
      "         [226, 221, 218,  ..., 245, 233, 228],\n",
      "         [224, 216, 230,  ..., 238, 222, 224],\n",
      "         [233, 232, 235,  ..., 233, 229, 241]]], dtype=torch.uint8), 'bboxes': [(5.800000000000001, 18.444444444444443, 86.60000000000001, 71.77777777777779, 1)]}{'image': tensor([[[221, 220, 218,  ..., 213, 210, 213],\n",
      "         [217, 216, 218,  ..., 213, 211, 212],\n",
      "         [220, 220, 217,  ..., 213, 211, 210],\n",
      "         ...,\n",
      "         [254, 253, 254,  ..., 213, 212, 211],\n",
      "         [254, 253, 254,  ..., 212, 212, 211],\n",
      "         [255, 254, 254,  ..., 212, 212, 212]],\n",
      "\n",
      "        [[ 35,  38,  40,  ..., 217, 218, 215],\n",
      "         [ 39,  41,  40,  ..., 217, 216, 217],\n",
      "         [ 40,  40,  41,  ..., 216, 217, 216],\n",
      "         ...,\n",
      "         [210, 209, 206,  ..., 220, 218, 217],\n",
      "         [210, 209, 207,  ..., 219, 218, 217],\n",
      "         [211, 210, 208,  ..., 218, 219, 218]],\n",
      "\n",
      "        [[ 40,  42,  40,  ..., 106, 107, 107],\n",
      "         [ 39,  40,  40,  ..., 106, 106, 106],\n",
      "         [ 39,  39,  39,  ..., 108, 106, 107],\n",
      "         ...,\n",
      "         [  1,   0,   0,  ..., 108, 108, 107],\n",
      "         [  1,   1,   1,  ..., 108, 106, 106],\n",
      "         [  2,   1,   1,  ..., 110, 103, 108]]], dtype=torch.uint8), 'bboxes': [(23.599999999999998, 13.813813813813812, 45.400000000000006, 82.58258258258259, 0)]}\n",
      "\n",
      "{'image': tensor([[[110, 112, 114,  ..., 110, 109, 105],\n",
      "         [110, 113, 114,  ..., 104, 102, 102],\n",
      "         [112, 113, 114,  ..., 101, 100,  99],\n",
      "         ...,\n",
      "         [ 95,  70,  28,  ..., 115, 115, 115],\n",
      "         [110,  24,  32,  ..., 117, 116, 116],\n",
      "         [ 42,  25,  29,  ..., 117, 118, 117]],\n",
      "\n",
      "        [[102, 103, 105,  ..., 103, 100,  98],\n",
      "         [103, 103, 105,  ...,  97,  93,  91],\n",
      "         [105, 106, 107,  ...,  89,  88,  90],\n",
      "         ...,\n",
      "         [109,  64,  44,  ..., 100, 100, 100],\n",
      "         [105,  39,  50,  ..., 102, 101, 101],\n",
      "         [ 44,  50,  47,  ..., 100, 101, 102]],\n",
      "\n",
      "        [[ 83,  88,  88,  ...,  85,  83,  82],\n",
      "         [ 84,  89,  88,  ...,  79,  76,  73],\n",
      "         [ 87,  88,  89,  ...,  73,  72,  73],\n",
      "         ...,\n",
      "         [ 44,  18,   6,  ...,  79,  77,  77],\n",
      "         [ 49,   2,  10,  ...,  79,  78,  78],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         [  3,  13,   7,  ...,  82,  81,  81]]], dtype=torch.uint8), 'bboxes': [(20.599999999999998, 8.009708737864079, 55.80000000000001, 41.99029126213592, 1)]}\n",
      "{'image': tensor([[[ 13,   7,   4,  ..., 254, 255, 252],\n",
      "         [ 16,   6,   1,  ..., 250, 251, 250],\n",
      "         [  5,   4,   1,  ..., 248, 249, 252],\n",
      "         ...,\n",
      "         [ 27,  54,  75,  ...,  73,  56,  55],\n",
      "         [ 55,  74,  73,  ...,  71,  55,  57],\n",
      "         [ 77,  78,  65,  ...,  58,  65,  60]],\n",
      "\n",
      "        [[  7,   1,   0,  ..., 219, 225, 224],\n",
      "         [ 10,   0,   0,  ..., 219, 224, 221],\n",
      "         [  2,   1,   0,  ..., 218, 221, 226],\n",
      "         ...,\n",
      "         [ 32,  84, 125,  ..., 112, 103, 100],\n",
      "         [ 85, 123, 123,  ..., 110, 103, 106],\n",
      "         [123, 124, 111,  ..., 101, 110, 104]],\n",
      "\n",
      "        [[ 17,  11,   9,  ...,  66,  79,  82],\n",
      "         [ 20,  10,   8,  ...,  69,  81,  83],\n",
      "         [ 11,  10,  10,  ...,  62,  77,  91],\n",
      "         ...,\n",
      "         [ 30,  38,  48,  ...,  17,  22,  17],\n",
      "         [ 47,  58,  50,  ...,  19,  17,  25],\n",
      "         [ 55,  52,  46,  ...,  16,  14,  25]]], dtype=torch.uint8), 'bboxes': [(41.6, 30.4, 12.399999999999999, 17.866666666666667, 0)]}\n",
      "[tensor([[[[0.3176, 0.3176, 0.3216,  ..., 0.3098, 0.3059, 0.3059],\n",
      "          [0.3216, 0.3176, 0.3255,  ..., 0.3098, 0.3098, 0.2980],\n",
      "          [0.3137, 0.3176, 0.3255,  ..., 0.3020, 0.2980, 0.3020],\n",
      "          ...,\n",
      "          [0.3412, 0.3373, 0.3451,  ..., 0.1176, 0.6902, 0.0000],\n",
      "          [0.3373, 0.3373, 0.3412,  ..., 0.6824, 0.0157, 0.0039],\n",
      "          [0.3373, 0.3373, 0.3412,  ..., 0.0118, 0.0000, 0.0039]],\n",
      "\n",
      "         [[0.5961, 0.5961, 0.6000,  ..., 0.5765, 0.5725, 0.5843],\n",
      "          [0.5961, 0.5961, 0.6039,  ..., 0.5922, 0.5804, 0.5725],\n",
      "          [0.5961, 0.5961, 0.6039,  ..., 0.5765, 0.5765, 0.5804],\n",
      "          ...,\n",
      "          [0.6275, 0.6235, 0.6314,  ..., 0.1176, 0.6902, 0.0039],\n",
      "          [0.6235, 0.6235, 0.6235,  ..., 0.6667, 0.0000, 0.0196],\n",
      "          [0.6235, 0.6235, 0.6275,  ..., 0.0392, 0.0196, 0.0667]],\n",
      "\n",
      "         [[0.9176, 0.9176, 0.9216,  ..., 0.9020, 0.8980, 0.9059],\n",
      "          [0.9176, 0.9176, 0.9255,  ..., 0.9137, 0.9020, 0.8941],\n",
      "          [0.9176, 0.9176, 0.9255,  ..., 0.9098, 0.8980, 0.9020],\n",
      "          ...,\n",
      "          [0.9373, 0.9333, 0.9333,  ..., 0.1176, 0.6902, 0.0196],\n",
      "          [0.9255, 0.9333, 0.9333,  ..., 0.6588, 0.0000, 0.0196],\n",
      "          [0.9255, 0.9333, 0.9373,  ..., 0.0667, 0.0471, 0.1451]]]]), {'boxes': tensor([[[13.9323, 23.6328, 84.5052, 68.9453]]]), 'labels': tensor([[0]]), 'image_id': tensor([[1]]), 'area': tensor([[3197.8354]]), 'iscrowd': tensor([[0]])}, tensor([[[[ 81,  81,  82,  ...,  79,  78,  78],\n",
      "          [ 82,  81,  83,  ...,  79,  79,  76],\n",
      "          [ 80,  81,  83,  ...,  77,  76,  77],\n",
      "          ...,\n",
      "          [ 87,  86,  88,  ...,  30, 176,   0],\n",
      "          [ 86,  86,  87,  ..., 174,   4,   1],\n",
      "          [ 86,  86,  87,  ...,   3,   0,   1]],\n",
      "\n",
      "         [[152, 152, 153,  ..., 147, 146, 149],\n",
      "          [152, 152, 154,  ..., 151, 148, 146],\n",
      "          [152, 152, 154,  ..., 147, 147, 148],\n",
      "          ...,\n",
      "          [160, 159, 161,  ...,  30, 176,   1],\n",
      "          [159, 159, 159,  ..., 170,   0,   5],\n",
      "          [159, 159, 160,  ...,  10,   5,  17]],\n",
      "\n",
      "         [[234, 234, 235,  ..., 230, 229, 231],\n",
      "          [234, 234, 236,  ..., 233, 230, 228],\n",
      "          [234, 234, 236,  ..., 232, 229, 230],\n",
      "          ...,\n",
      "          [239, 238, 238,  ...,  30, 176,   5],\n",
      "          [236, 238, 238,  ..., 168,   0,   5],\n",
      "          [236, 238, 239,  ...,  17,  12,  37]]]], dtype=torch.uint8), tensor([[[13.9323, 23.6328, 84.5052, 68.9453]]])]\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "lr = 0.05\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters() , lr, momentum = 0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "losses = []\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i,data in enumerate(dl_train):\n",
    "        print(data)\n",
    "        break\n",
    "    break\n",
    "#         inputs = data['image']\n",
    "#         print(inputs)\n",
    "        #inputs,label = data[0].to(device),data[1][0].to(device)\n",
    "        #print(inputs ,'hiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiihjsd')\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(inputs)\n",
    "#         loss = criterion(output,label)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         losses.append(loss.item())\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77d13429",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': tensor([[[ 47,  49,  48,  ...,  51,  51,  65],\n",
      "         [ 49,  51,  51,  ...,  42,  45,  59],\n",
      "         [ 55,  55,  57,  ...,  30,  39,  52],\n",
      "         ...,\n",
      "         [102,  91,  78,  ...,  79, 127, 154],\n",
      "         [108,  97,  83,  ..., 149, 149, 144],\n",
      "         [115, 104,  88,  ..., 130, 130, 124]],\n",
      "\n",
      "        [[141, 144, 142,  ..., 130, 131, 135],\n",
      "         [143, 145, 145,  ..., 123, 124, 130],\n",
      "         [146, 146, 148,  ..., 114, 117, 127],\n",
      "         ...,\n",
      "         [ 92,  84,  75,  ...,  68, 119, 152],\n",
      "         [ 96,  88,  78,  ..., 147, 147, 147],\n",
      "         [101,  95,  81,  ..., 136, 134, 133]],\n",
      "\n",
      "        [[143, 146, 148,  ..., 130, 127, 135],\n",
      "         [143, 145, 149,  ..., 130, 129, 132],\n",
      "         [147, 147, 149,  ..., 125, 127, 130],\n",
      "         ...,\n",
      "         [ 67,  66,  60,  ...,  62,  94, 114],\n",
      "         [ 72,  71,  64,  ..., 125, 128, 124],\n",
      "         [ 78,  78,  67,  ..., 118, 123, 123]]], dtype=torch.uint8), 'bboxes': [(46.800000000000004, 29.83425414364641, 24.79999999999999, 27.071823204419886, 0)]}\n",
      "{'image': tensor([[[ 98,  96,  90,  ..., 188, 185, 183],\n",
      "         [112, 110, 108,  ..., 189, 185, 185],\n",
      "         [124, 125, 125,  ..., 189, 185, 185],\n",
      "         ...,\n",
      "         [ 39,  37,  34,  ...,  71,  72,  78],\n",
      "         [ 35,  36,  36,  ...,  70,  73,  78],\n",
      "         [ 30,  31,  36,  ...,  69,  71,  77]],\n",
      "\n",
      "        [[ 83,  81,  75,  ..., 171, 169, 168],\n",
      "         [ 98,  96,  91,  ..., 172, 169, 169],\n",
      "         [109, 110, 106,  ..., 171, 169, 169],\n",
      "         ...,\n",
      "         [ 30,  27,  27,  ..., 130, 131, 134],\n",
      "         [ 26,  27,  29,  ..., 129, 131, 132],\n",
      "         [ 25,  26,  29,  ..., 129, 129, 131]],\n",
      "\n",
      "        [[ 78,  76,  70,  ..., 164, 170, 168],\n",
      "         [ 89,  87,  83,  ..., 165, 170, 169],\n",
      "         [102, 103, 100,  ..., 169, 169, 169],\n",
      "         ...,\n",
      "         [ 22,  20,  21,  ..., 144, 142, 145],\n",
      "         [ 19,  20,  23,  ..., 143, 143, 144],\n",
      "         [ 21,  22,  23,  ..., 140, 141, 143]]], dtype=torch.uint8), 'bboxes': [(33.800000000000004, 33.60433604336043, 29.599999999999994, 20.596205962059614, 0), (3.0, 57.45257452574526, 92.39999999999999, 38.75338753387534, 0)]}\n",
      "{'image': tensor([[[ 98, 105, 112,  ...,  92,  92,  92],\n",
      "         [ 98, 105, 113,  ...,  90,  91,  87],\n",
      "         [103, 106, 112,  ...,  95,  91,  90],\n",
      "         ...,\n",
      "         [163, 167, 163,  ..., 125, 126, 126],\n",
      "         [161, 161, 162,  ..., 125, 124, 122],\n",
      "         [153, 156, 155,  ..., 122, 121, 117]],\n",
      "\n",
      "        [[121, 124, 129,  ..., 110, 105,  99],\n",
      "         [124, 127, 131,  ..., 110, 107, 107],\n",
      "         [126, 132, 134,  ..., 112, 108, 107],\n",
      "         ...,\n",
      "         [159, 158, 159,  ..., 126, 125, 126],\n",
      "         [157, 156, 155,  ..., 124, 123, 121],\n",
      "         [152, 155, 156,  ..., 121, 120, 116]],\n",
      "\n",
      "        [[107, 108, 114,  ...,  98,  95,  91],\n",
      "         [113, 114, 119,  ...,  98,  96,  98],\n",
      "         [118, 119, 121,  ..., 104, 100,  99],\n",
      "         ...,\n",
      "         [148, 149, 150,  ..., 120, 120, 118],\n",
      "         [145, 150, 149,  ..., 119, 118, 116],\n",
      "         [143, 144, 148,  ..., 116, 116, 111]]], dtype=torch.uint8), 'bboxes': [(13.600000000000001, 20.266666666666666, 12.399999999999999, 31.46666666666667, 0), (33.800000000000004, 20.8, 8.199999999999996, 21.599999999999998, 0), (29.4, 38.93333333333333, 8.600000000000001, 20.53333333333334, 0), (42.8, 15.2, 29.200000000000003, 64.8, 0), (77.8, 40.266666666666666, 13.600000000000009, 13.866666666666667, 0)]}{'image': tensor([[[ 22,  14,  17,  ...,   8,  11,  10],\n",
      "         [ 15,  26,  16,  ...,   7,   5,  14],\n",
      "         [ 20,  10,  18,  ...,   8,  10,  10],\n",
      "         ...,\n",
      "         [170, 184, 173,  ..., 184, 178, 187],\n",
      "         [173, 165, 183,  ..., 187, 175, 193],\n",
      "         [181, 184, 183,  ..., 191, 173, 192]],\n",
      "\n",
      "        [[ 23,  19,  19,  ...,  22,  10,  13],\n",
      "         [ 25,  23,  18,  ...,  14,  15,  11],\n",
      "         [ 18,  21,  18,  ...,  11,  16,  12],\n",
      "         ...,\n",
      "         [206, 207, 199,  ..., 233, 209, 217],\n",
      "         [206, 213, 217,  ..., 231, 207, 211],\n",
      "         [215, 223, 213,  ..., 224, 215, 226]],\n",
      "\n",
      "        [[ 19,  19,  18,  ...,  14,   8,   8],\n",
      "         [ 22,  15,  13,  ...,  12,  11,   8],\n",
      "         [ 16,  17,  14,  ...,  11,   9,  13],\n",
      "         ...,\n",
      "         [226, 221, 218,  ..., 245, 233, 228],\n",
      "         [224, 216, 230,  ..., 238, 222, 224],\n",
      "         [233, 232, 235,  ..., 233, 229, 241]]], dtype=torch.uint8), 'bboxes': [(5.800000000000001, 18.444444444444443, 86.60000000000001, 71.77777777777779, 1)]}{'image': tensor([[[ 81,  81,  82,  ...,  79,  78,  78],\n",
      "         [ 82,  81,  83,  ...,  79,  79,  76],\n",
      "         [ 80,  81,  83,  ...,  77,  76,  77],\n",
      "         ...,\n",
      "         [ 87,  86,  88,  ...,  30, 176,   0],\n",
      "         [ 86,  86,  87,  ..., 174,   4,   1],\n",
      "         [ 86,  86,  87,  ...,   3,   0,   1]],\n",
      "\n",
      "        [[152, 152, 153,  ..., 147, 146, 149],\n",
      "         [152, 152, 154,  ..., 151, 148, 146],\n",
      "         [152, 152, 154,  ..., 147, 147, 148],\n",
      "         ...,\n",
      "         [160, 159, 161,  ...,  30, 176,   1],\n",
      "         [159, 159, 159,  ..., 170,   0,   5],\n",
      "         [159, 159, 160,  ...,  10,   5,  17]],\n",
      "\n",
      "        [[234, 234, 235,  ..., 230, 229, 231],\n",
      "         [234, 234, 236,  ..., 233, 230, 228],\n",
      "         [234, 234, 236,  ..., 232, 229, 230],\n",
      "         ...,\n",
      "         [239, 238, 238,  ...,  30, 176,   5],\n",
      "         [236, 238, 238,  ..., 168,   0,   5],\n",
      "         [236, 238, 239,  ...,  17,  12,  37]]], dtype=torch.uint8), 'bboxes': [(13.932291666666666, 23.6328125, 70.57291666666667, 45.3125, 0)]}{'image': tensor([[[110, 112, 114,  ..., 110, 109, 105],\n",
      "         [110, 113, 114,  ..., 104, 102, 102],\n",
      "         [112, 113, 114,  ..., 101, 100,  99],\n",
      "         ...,\n",
      "         [ 95,  70,  28,  ..., 115, 115, 115],\n",
      "         [110,  24,  32,  ..., 117, 116, 116],\n",
      "         [ 42,  25,  29,  ..., 117, 118, 117]],\n",
      "\n",
      "        [[102, 103, 105,  ..., 103, 100,  98],\n",
      "         [103, 103, 105,  ...,  97,  93,  91],\n",
      "         [105, 106, 107,  ...,  89,  88,  90],\n",
      "         ...,\n",
      "         [109,  64,  44,  ..., 100, 100, 100],\n",
      "         [105,  39,  50,  ..., 102, 101, 101],\n",
      "         [ 44,  50,  47,  ..., 100, 101, 102]],\n",
      "\n",
      "        [[ 83,  88,  88,  ...,  85,  83,  82],\n",
      "         [ 84,  89,  88,  ...,  79,  76,  73],\n",
      "         [ 87,  88,  89,  ...,  73,  72,  73],\n",
      "         ...,\n",
      "         [ 44,  18,   6,  ...,  79,  77,  77],\n",
      "         [ 49,   2,  10,  ...,  79,  78,  78],\n",
      "         [  3,  13,   7,  ...,  82,  81,  81]]], dtype=torch.uint8), 'bboxes': [(20.599999999999998, 8.009708737864079, 55.80000000000001, 41.99029126213592, 1)]}\n",
      "\n",
      "\n",
      "\n",
      "{'image': tensor([[[205, 213, 205,  ..., 101,  96,  79],\n",
      "         [188, 209, 194,  ..., 120, 105,  95],\n",
      "         [192, 188, 183,  ..., 133, 121, 114],\n",
      "         ...,\n",
      "         [191, 149, 135,  ..., 229, 192, 248],\n",
      "         [191, 159, 148,  ..., 232, 219, 222],\n",
      "         [204, 189, 189,  ..., 216, 235, 216]],\n",
      "\n",
      "        [[196, 199, 189,  ...,  92,  90,  82],\n",
      "         [179, 191, 182,  ..., 107,  93,  92],\n",
      "         [173, 176, 177,  ..., 120, 111, 106],\n",
      "         ...,\n",
      "         [166, 130, 119,  ..., 219, 184, 239],\n",
      "         [166, 143, 128,  ..., 219, 206, 211],\n",
      "         [181, 175, 167,  ..., 201, 220, 202]],\n",
      "\n",
      "        [[155, 164, 147,  ...,  56,  54,  53],\n",
      "         [131, 150, 142,  ...,  70,  59,  57],\n",
      "         [123, 134, 139,  ...,  77,  73,  70],\n",
      "         ...,\n",
      "         [125,  93,  86,  ..., 192, 150, 203],\n",
      "         [124, 102,  91,  ..., 181, 167, 176],\n",
      "         [140, 126, 126,  ..., 159, 181, 165]]], dtype=torch.uint8), 'bboxes': [(47.8, 30.930930930930934, 35.8, 57.35735735735735, 0)]}\n",
      "{'image': tensor([[[221, 220, 218,  ..., 213, 210, 213],\n",
      "         [217, 216, 218,  ..., 213, 211, 212],\n",
      "         [220, 220, 217,  ..., 213, 211, 210],\n",
      "         ...,\n",
      "         [254, 253, 254,  ..., 213, 212, 211],\n",
      "         [254, 253, 254,  ..., 212, 212, 211],\n",
      "         [255, 254, 254,  ..., 212, 212, 212]],\n",
      "\n",
      "        [[ 35,  38,  40,  ..., 217, 218, 215],\n",
      "         [ 39,  41,  40,  ..., 217, 216, 217],\n",
      "         [ 40,  40,  41,  ..., 216, 217, 216],\n",
      "         ...,\n",
      "         [210, 209, 206,  ..., 220, 218, 217],\n",
      "         [210, 209, 207,  ..., 219, 218, 217],\n",
      "         [211, 210, 208,  ..., 218, 219, 218]],\n",
      "\n",
      "        [[ 40,  42,  40,  ..., 106, 107, 107],\n",
      "         [ 39,  40,  40,  ..., 106, 106, 106],\n",
      "         [ 39,  39,  39,  ..., 108, 106, 107],\n",
      "         ...,\n",
      "         [  1,   0,   0,  ..., 108, 108, 107],\n",
      "         [  1,   1,   1,  ..., 108, 106, 106],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         [  2,   1,   1,  ..., 110, 103, 108]]], dtype=torch.uint8), 'bboxes': [(23.599999999999998, 13.813813813813812, 45.400000000000006, 82.58258258258259, 0)]}\n",
      "{'image': tensor([[[ 13,   7,   4,  ..., 254, 255, 252],\n",
      "         [ 16,   6,   1,  ..., 250, 251, 250],\n",
      "         [  5,   4,   1,  ..., 248, 249, 252],\n",
      "         ...,\n",
      "         [ 27,  54,  75,  ...,  73,  56,  55],\n",
      "         [ 55,  74,  73,  ...,  71,  55,  57],\n",
      "         [ 77,  78,  65,  ...,  58,  65,  60]],\n",
      "\n",
      "        [[  7,   1,   0,  ..., 219, 225, 224],\n",
      "         [ 10,   0,   0,  ..., 219, 224, 221],\n",
      "         [  2,   1,   0,  ..., 218, 221, 226],\n",
      "         ...,\n",
      "         [ 32,  84, 125,  ..., 112, 103, 100],\n",
      "         [ 85, 123, 123,  ..., 110, 103, 106],\n",
      "         [123, 124, 111,  ..., 101, 110, 104]],\n",
      "\n",
      "        [[ 17,  11,   9,  ...,  66,  79,  82],\n",
      "         [ 20,  10,   8,  ...,  69,  81,  83],\n",
      "         [ 11,  10,  10,  ...,  62,  77,  91],\n",
      "         ...,\n",
      "         [ 30,  38,  48,  ...,  17,  22,  17],\n",
      "         [ 47,  58,  50,  ...,  19,  17,  25],\n",
      "         [ 55,  52,  46,  ...,  16,  14,  25]]], dtype=torch.uint8), 'bboxes': [(41.6, 30.4, 12.399999999999999, 17.866666666666667, 0)]}\n",
      "[tensor([[[[0.3176, 0.3176, 0.3216,  ..., 0.3098, 0.3059, 0.3059],\n",
      "          [0.3216, 0.3176, 0.3255,  ..., 0.3098, 0.3098, 0.2980],\n",
      "          [0.3137, 0.3176, 0.3255,  ..., 0.3020, 0.2980, 0.3020],\n",
      "          ...,\n",
      "          [0.3412, 0.3373, 0.3451,  ..., 0.1176, 0.6902, 0.0000],\n",
      "          [0.3373, 0.3373, 0.3412,  ..., 0.6824, 0.0157, 0.0039],\n",
      "          [0.3373, 0.3373, 0.3412,  ..., 0.0118, 0.0000, 0.0039]],\n",
      "\n",
      "         [[0.5961, 0.5961, 0.6000,  ..., 0.5765, 0.5725, 0.5843],\n",
      "          [0.5961, 0.5961, 0.6039,  ..., 0.5922, 0.5804, 0.5725],\n",
      "          [0.5961, 0.5961, 0.6039,  ..., 0.5765, 0.5765, 0.5804],\n",
      "          ...,\n",
      "          [0.6275, 0.6235, 0.6314,  ..., 0.1176, 0.6902, 0.0039],\n",
      "          [0.6235, 0.6235, 0.6235,  ..., 0.6667, 0.0000, 0.0196],\n",
      "          [0.6235, 0.6235, 0.6275,  ..., 0.0392, 0.0196, 0.0667]],\n",
      "\n",
      "         [[0.9176, 0.9176, 0.9216,  ..., 0.9020, 0.8980, 0.9059],\n",
      "          [0.9176, 0.9176, 0.9255,  ..., 0.9137, 0.9020, 0.8941],\n",
      "          [0.9176, 0.9176, 0.9255,  ..., 0.9098, 0.8980, 0.9020],\n",
      "          ...,\n",
      "          [0.9373, 0.9333, 0.9333,  ..., 0.1176, 0.6902, 0.0196],\n",
      "          [0.9255, 0.9333, 0.9333,  ..., 0.6588, 0.0000, 0.0196],\n",
      "          [0.9255, 0.9333, 0.9373,  ..., 0.0667, 0.0471, 0.1451]]]]), {'boxes': tensor([[[13.9323, 23.6328, 84.5052, 68.9453]]]), 'labels': tensor([[0]]), 'image_id': tensor([[1]]), 'area': tensor([[3197.8354]]), 'iscrowd': tensor([[0]])}]\n"
     ]
    }
   ],
   "source": [
    "for k in dl_train:\n",
    "    print(k)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff6a29d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
